{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85T3ZUeFCppb",
    "outputId": "0598a962-f850-480e-8b0a-8f5240757c23"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to movie reviews zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgb5trmdGwFN"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"sentiment-analysis-on-movie-reviews.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "with zipfile.ZipFile(\"train.tsv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "with zipfile.ZipFile(\"test.tsv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d60DdUxWZY_A"
   },
   "outputs": [],
   "source": [
    "import torch, os, random, zipfile, re, pickle, logging\n",
    "import numpy as np, torch.nn as nn, pandas as pd,\\\n",
    "torch.nn.functional as F, matplotlib.pyplot as plt,\\\n",
    "seaborn as sn\n",
    "from time import time, sleep\n",
    "from models import BertForSentiment\n",
    "from torch.optim import Adam\n",
    "from string import punctuation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, \n",
    "    DataLoader, \n",
    "    Subset, \n",
    "    RandomSampler, \n",
    "    SequentialSampler, \n",
    "    Dataset\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAmZiKHfe7MZ"
   },
   "outputs": [],
   "source": [
    "class SelfDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, max_len):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-uncased', do_lower_case=True\n",
    "        )\n",
    "        self.inputs = inputs\n",
    "        self.labels = torch.as_tensor(\n",
    "            labels, dtype=torch.long\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_dict = self.tokenizer(\n",
    "            self.inputs[item],\n",
    "            add_special_tokens = True,\n",
    "            truncation=True,\n",
    "            max_length = self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "        )\n",
    "        return (\n",
    "            encoded_dict['input_ids'].view(-1), \n",
    "            encoded_dict['attention_mask'].view(-1), \n",
    "            self.labels[item]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.size()[0]\n",
    "\n",
    "class TrainTest():\n",
    "    def __init__(self, model, optimizer, scheduler, criterion, logger=None, method='cls'):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.logger = logger\n",
    "        self.method = method\n",
    "        self.tr_metrics = {\n",
    "            'train_loss':[],\n",
    "            'train_accuracy':[],\n",
    "            'valid_loss':[],\n",
    "            'valid_accuracy':[],\n",
    "        }\n",
    "        \n",
    "    def train(\n",
    "        self, train_loader, valid_loader, \n",
    "        num_epochs, device, eval_interval,\n",
    "        clip=None, model_path=None, save_per_epoch=None,\n",
    "        results_path=None, defaults=None, **kwargs\n",
    "    ):\n",
    "        total_itrs = num_epochs*len(train_loader)\n",
    "        num_tr, total_tr_loss, itr = 0, 0, 0\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (input_ids, att_masks, input_labels) in enumerate(train_loader):\n",
    "                input_ids = input_ids.to(device)\n",
    "                att_masks = att_masks.to(device)\n",
    "                input_labels = input_labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(\n",
    "                    input_ids,\n",
    "                    attention_mask=att_masks,\n",
    "                    token_type_ids=None,\n",
    "                    method=self.method\n",
    "                )\n",
    "                tr_loss = self.criterion(output, input_labels.view(-1))\n",
    "                # nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                if self.logger:\n",
    "                    self.logger.info(f'Training: {itr}/{total_itrs} -- loss: {tr_loss.item()}')\n",
    "                tr_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                num_tr += 1\n",
    "                total_tr_loss += tr_loss\n",
    "                if itr % eval_interval == 0 or itr+1 == total_itrs:\n",
    "                    self.tr_metrics['train_loss'].append(total_tr_loss.cpu().item()/num_tr)\n",
    "                    tr_accuracy = cal_accuracy(output, input_labels)\n",
    "                    self.tr_metrics['train_accuracy'].append(tr_accuracy)\n",
    "                    num_tr, total_tr_loss = 0, 0\n",
    "                    val_loss = 0\n",
    "                    self.model.eval()\n",
    "                    val_accuracy = []\n",
    "                    with torch.no_grad():\n",
    "                        for val_ids, val_masks, val_labels in valid_loader:\n",
    "                            val_ids = val_ids.to(device)\n",
    "                            val_masks = val_masks.to(device)\n",
    "                            val_labels = val_labels.to(device)\n",
    "                            val_output = self.model(\n",
    "                                val_ids,\n",
    "                                attention_mask=val_masks,\n",
    "                                token_type_ids=None,\n",
    "                                method=self.method\n",
    "                            )\n",
    "                            val_loss += self.criterion(val_output, val_labels.view(-1))\n",
    "                            val_accuracy.append(cal_accuracy(val_output, val_labels))\n",
    "                    self.tr_metrics['valid_accuracy'].append(np.mean(val_accuracy))\n",
    "                    self.tr_metrics['valid_loss'].append(val_loss.cpu().item()/len(valid_loader))\n",
    "                    self.model.train()\n",
    "                    if self.logger:\n",
    "                        self.logger.info(f'Training: iteration: {itr}/{total_itrs} -- epoch: {epoch} -- '\n",
    "                        f' train_loss: {self.tr_metrics[\"train_loss\"][-1]:.3f} -- train_accuracy: {self.tr_metrics[\"train_accuracy\"][-1]:.2f}'\n",
    "                        f' valid_loss: {self.tr_metrics[\"valid_loss\"][-1]:.3f} -- valid_accuracy: {self.tr_metrics[\"valid_accuracy\"][-1]:.2f}')\n",
    "                itr += 1\n",
    "            if model_path and results_path and ((epoch+1) % save_per_epoch == 0) and epoch != 0:\n",
    "                self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_train')\n",
    "                self.save_results(results_path, f'{epoch+1}_epochs_train', self.tr_metrics)\n",
    "        if model_path and results_path:\n",
    "            self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_last_train')\n",
    "            self.save_results(results_path, f'{epoch+1}_epochs_last_train', self.tr_metrics)\n",
    "            \n",
    "    def save_model(self, epoch, model_path, name):\n",
    "        model_dir = '/'.join(model_path.split('/')[:-1])\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'loss': self.tr_metrics['valid_loss'][-1],\n",
    "            }, os.path.join(model_dir, f'model_{name}.pt')\n",
    "        )\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: model saved to: {model_dir}/model_{name}.pt')\n",
    "    \n",
    "    def save_results(self, results_path, name, results):\n",
    "        results_dir = '/'.join(results_path.split('/')[:-1])\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        with open(os.path.join(results_dir, f'results_{name}.pkl'), 'wb') as save_file:\n",
    "            pickle.dump(results, save_file)\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: results saved to: {results_dir}/resutls_{name}.pkl')\n",
    "\n",
    "    def test(test_loader, device, all_labels, results_path=None, defaults=None):\n",
    "        test_accuracy, test_true, test_pred = [], [], []\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_ids, test_masks, test_labels in test_loader:\n",
    "                test_ids = test_ids.to(device)\n",
    "                test_masks = test_masks.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "                test_output = self.model(\n",
    "                    test_ids,\n",
    "                    attention_mask=test_masks,\n",
    "                    token_type_ids=None,\n",
    "                    method=self.method\n",
    "                )\n",
    "                test_loss += self.criterion(test_output, test_labels.view(-1))\n",
    "                test_accuracy.append(cal_accuracy(test_output, test_labels))\n",
    "                test_true.append(test_labels.cpu())\n",
    "                test_pred.append(test_output.cpu().max(dim=1)[1])\n",
    "        test_true = torch.cat(test_true)\n",
    "        test_pred = torch.cat(test_pred)\n",
    "        test_loss = test_loss.cpu().item()/len(test_loader)\n",
    "        test_accuracy = np.mean(test_accuracy)\n",
    "        prf = precision_recall_fscore_support(\n",
    "            test_true,\n",
    "            test_pred,\n",
    "            labels=all_labels,\n",
    "            average='weighted'\n",
    "        )\n",
    "        confm = confusion_matrix(test_true, test_pred, labels=all_labels)\n",
    "        self.ts_metrics = {\n",
    "            'loss':test_loss,\n",
    "            'accuracy':test_accuracy,\n",
    "            'precision':prf[0],\n",
    "            'recall':prf[1],\n",
    "            'f1_score':prf[2],\n",
    "            'confusion_matrix':confm\n",
    "        }\n",
    "        if self.logger:\n",
    "            print(f'Testing: test_loss: {test_loss:.3f} -- test_accurcy: {test_accuracy:.2f}')\n",
    "        if results_path:\n",
    "            self.save_results(results_path, f'test', self.ts_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOmRQzHry_Eg"
   },
   "outputs": [],
   "source": [
    "def download_dirs(dirlist):\n",
    "    from google.colab import files\n",
    "    for dirname in dirlist:\n",
    "        for filename in os.listdir(dirname):\n",
    "            filename = os.path.join(dirname, filename)\n",
    "            files.download(filename)\n",
    "            \n",
    "def get_dataLoader(dataset, splits, batch_sizes, shuffle_indices, **kwargs):\n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    if shuffle_indices:\n",
    "        np.random.shuffle(indices)\n",
    "    split_index = int(n_samples*splits[0])\n",
    "    train_indices = indices[:split_index]\n",
    "    valid_indices = indices[split_index:]\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, shuffle=True, batch_size=batch_sizes[0], **kwargs\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, shuffle=False, batch_size=batch_sizes[1], **kwargs\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def fix_seed(seed_value, random_lib=False, numpy_lib=False, torch_lib=False):\n",
    "    if random_lib:\n",
    "        random.seed(seed_value)\n",
    "    if numpy_lib:\n",
    "        np.random.seed(seed_value)\n",
    "    if torch_lib:\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def setup_logger(name, format=None, level=logging.DEBUG, handlers=None, log_file='default.log'):\n",
    "    logging.basicConfig(\n",
    "        level=level, \n",
    "        format=format if format else '%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=handlers if handlers else [\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "def cal_accuracy(pred_labels, true_labels):\n",
    "    _, pred_labels = pred_labels.max(dim=1)\n",
    "    true_labels = true_labels.view(-1)\n",
    "    return torch.sum(pred_labels == true_labels).item() / true_labels.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsVoIVuke7Me"
   },
   "outputs": [],
   "source": [
    "cuda_flag = True if torch.cuda.is_available() else False\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "n_iters = 10000\n",
    "num_epochs = 1\n",
    "eval_interval = 150\n",
    "save_model = True\n",
    "device = torch.device('cuda' if cuda_flag else 'cpu')\n",
    "params = {'num_workers': 2, 'pin_memory': True} if cuda_flag else {}\n",
    "data_splits = [0.85, 0.15]\n",
    "batch_sizes = [64, 64]\n",
    "seed = 20214\n",
    "fix_seed(seed, random_lib=True, numpy_lib=True, torch_lib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEZQnfgfe7Mi"
   },
   "source": [
    "# Set path to train .tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRDTml4gBk0X"
   },
   "outputs": [],
   "source": [
    "tr_phrases = pd.read_csv(\n",
    "    'train.tsv', \n",
    "    sep='\\t', engine='python', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to GloVe 300d embedding fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJ5GTWBrRJed"
   },
   "outputs": [],
   "source": [
    "augment = False\n",
    "aug_method = 'embedding'\n",
    "if augment:\n",
    "    tr_phrases['len_sent'] = tr_phrases.Phrase.apply(lambda x: len(x.split()))\n",
    "    cand_df = tr_phrases[tr_phrases.len_sent>5].sample(int(len(tr_phrases)*0.1), random_state=2020).reset_index(drop=True)\n",
    "    \n",
    "    if aug_method == 'embedding':\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        from string import punctuation\n",
    "        embedded_words = dict()\n",
    "        glove_path = '/content/drive/My Drive/GLOVE/glove.42B.300d.txt'\n",
    "        glove_file = open(glove_path)\n",
    "        for line in glove_file:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            words_weights = np.array(values[1:], dtype='float32')\n",
    "            embedded_words[word] = words_weights\n",
    "        glove_file.close()\n",
    "        mean_embedding = np.array(list(embedded_words.values())).mean(axis=0)\n",
    "        values = []\n",
    "        keys = []\n",
    "        for key, value in embedded_words.items():\n",
    "            keys.append(key)\n",
    "            values.append(value)\n",
    "        neigh = NearestNeighbors(n_neighbors=2, radius=0.4)\n",
    "        neigh.fit(values)\n",
    "    elif aug_method == 'synonym':\n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet\n",
    "        all_words = list(wordnet.all_lemma_names(pos='n', lang='eng'))\n",
    "\n",
    "    def word_replace(sent, method):\n",
    "        sent = sent.lower()\n",
    "        sent = sent.split()\n",
    "        clean_sent = [word for word in sent if word not in punctuation]\n",
    "        if method == 'embedding':\n",
    "            cand_word = np.random.choice(clean_sent)\n",
    "            try:\n",
    "                cand_embed = embedded_words[cand_word]\n",
    "            except:\n",
    "                print('word not found')\n",
    "                cand_embed = mean_embedding\n",
    "            alt_word_idx = neigh.kneighbors(cand_embed.reshape(1, -1), 2, return_distance=False)[0, 1]\n",
    "            alt_word = keys[alt_word_idx]\n",
    "        elif method == 'synonym':\n",
    "            not_found = True\n",
    "            alt_word = np.random.choice(all_words)\n",
    "            checked_words = 0\n",
    "            while not_found and checked_words < 2*len(clean_sent):\n",
    "                cand_word = np.random.choice(clean_sent)\n",
    "                checked_words += 1\n",
    "                if cand_word not in all_words:\n",
    "                    continue\n",
    "                syns = wordnet.synsets(cand_word)\n",
    "                for syn in syns:\n",
    "                    syn_word = syn.lemmas()[0].name()\n",
    "                    if syn_word != cand_word:\n",
    "                        not_found = False\n",
    "                        alt_word = syn_word\n",
    "                        break\n",
    "        sent[sent.index(cand_word)] = alt_word\n",
    "        sent = ' '.join(sent)\n",
    "        return sent\n",
    "\n",
    "    cand_df['Phrase'] = cand_df.Phrase.apply(lambda x: word_replace(x, aug_method))\n",
    "    tr_phrases = pd.concat([tr_phrases, cand_df], ignore_index=True)\n",
    "    aug_path = './augmented_train.csv'\n",
    "    tr_phrases.to_csv(aug_path, sep='\\t', header=True)\n",
    "    tr_file_path = aug_path\n",
    "else:\n",
    "    tr_file_path = 'train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moWeMTdzRESJ"
   },
   "outputs": [],
   "source": [
    "single_sentence = True\n",
    "if single_sentence:\n",
    "    max_pad = 100\n",
    "else:\n",
    "    tr_phrases['Phrase'] = tr_phrases.Phrase.apply(lambda x: x+' [SEP] '+x)\n",
    "    max_pad = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqAAT1ylRFHu",
    "outputId": "971e7f1c-b86b-4b9b-c0d8-ec0c86248114"
   },
   "outputs": [],
   "source": [
    "tr_dataset = SelfDataset(\n",
    "    inputs=tr_phrases.Phrase.values,\n",
    "    labels=tr_phrases.Sentiment.values,\n",
    "    max_len=max_pad\n",
    ")\n",
    "train_loader, valid_loader = get_dataLoader(\n",
    "    tr_dataset,\n",
    "    data_splits,\n",
    "    batch_sizes,\n",
    "    shuffle_indices=True, **params\n",
    ")\n",
    "print(f'number of epochs: {num_epochs}')\n",
    "print(f'number of iteration: {num_epochs*len(train_loader)}')\n",
    "print(f'number of intervals: {num_epochs*len(train_loader)/eval_interval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ds4SLkTeGBfH",
    "outputId": "44767913-6f24-4fca-8e7c-ab308dcaeb67"
   },
   "outputs": [],
   "source": [
    "vocab_file = 'bert-base-uncased'\n",
    "model = BertForSentiment(vocab_file, num_classes=5, contain_cls=True).to(device)\n",
    "optimizer = Adam([\n",
    "#     {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "    {\"params\":model.dropout.parameters(), \"lr\": lrlast},\n",
    "    {\"params\":model.classifier.parameters(), \"lr\": lrlast},                    \n",
    "])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = 2000\n",
    ")\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "open('metrics.log', 'w').close()\n",
    "logger = setup_logger(name='track_logger', log_file='metrics.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3CSt7o7GCsl",
    "outputId": "6c47ead7-6a19-42e6-f8ad-f7eceee168da"
   },
   "outputs": [],
   "source": [
    "traintest = TrainTest(model, optimizer, scheduler, F.nll_loss, logger, method='avg')\n",
    "traintest.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    eval_interval,\n",
    "    model_path='./models/',\n",
    "    save_per_epoch=10,\n",
    "    results_path='./results/',\n",
    "    clip=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "B4rulcgpwSeH",
    "outputId": "74211f49-39d1-42de-ea47-3b5f1580b733"
   },
   "outputs": [],
   "source": [
    "download_dirs(['./results/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bfumrm1wS8nP",
    "outputId": "d6ee967b-93e7-40b5-9db6-bfa012d54ae2"
   },
   "outputs": [],
   "source": [
    "traintest.tr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml3QwBmmTZ0M"
   },
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    results_dir, train_results_files, eval_interval, variations,\n",
    "    loss_img_name='', accuracy_img_name='', **kwargs\n",
    "):\n",
    "    results = []\n",
    "    for i in range(len(train_results_files)):\n",
    "        with open(os.path.join(results_dir, train_results_files[i]), 'rb') as results_file:\n",
    "            train_results = pickle.load(results_file)\n",
    "            iterations = [i*eval_interval for i in range(len(train_results['train_loss']))]\n",
    "            results.append((train_results, variations[i], iterations))\n",
    "    plt.figure()\n",
    "    for result in results:\n",
    "        plt.plot(result[2], result[0]['train_loss'], label=f'train loss {result[1]}')\n",
    "        plt.plot(result[2], result[0]['valid_loss'], label=f'valid loss {result[1]}')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, loss_img_name+'train-validation-loss.png'), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.figure()\n",
    "    for result in results:\n",
    "        plt.plot(result[2], result[0]['train_accuracy'], label=f'train accuracy {result[1]}')\n",
    "        plt.plot(result[2], result[0]['valid_accuracy'], label=f'valid accuracy {result[1]}')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, accuracy_img_name+'train-validation-accuracy.png'), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'sentiment-results'\n",
    "train_results_files = [\n",
    "    'resutls_one_sent_input_cls.pkl', 'results_two_sent_input_cls.pkl'\n",
    "]\n",
    "variations = ['one sent', 'two sent']\n",
    "eval_interval = 150\n",
    "report_results(\n",
    "    results_dir, train_results_files, eval_interval, variations,\n",
    "    loss_img_name='input_', accuracy_img_name='input_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'sentiment-results'\n",
    "train_results_files = [\n",
    "    'resutls_one_sent_input_cls.pkl',\n",
    "    'resutls_one_sent_input_max.pkl',\n",
    "    'resutls_one_sent_input_avg.pkl',\n",
    "    'resutls_one_sent_input_avg_cls.pkl'\n",
    "]\n",
    "variations = ['cls', 'max', 'avg', 'avg-cls']\n",
    "eval_interval = 150\n",
    "report_results(\n",
    "    results_dir, train_results_files, eval_interval, variations,\n",
    "    loss_img_name='output_', accuracy_img_name='output_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'sentiment-results'\n",
    "train_results_files = [\n",
    "    'resutls_one_sent_input_avg_cls.pkl',\n",
    "    'resutls_one_sent_input_avg_cls_embedding_augment.pkl',\n",
    "    'resutls_one_sent_input_avg_cls_synonym_augment.pkl',\n",
    "]\n",
    "variations = ['main', 'GloVe augmentation', 'WordNet augmentation']\n",
    "eval_interval = 150\n",
    "report_results(\n",
    "    results_dir, train_results_files, eval_interval, variations,\n",
    "    loss_img_name='augmentation_', accuracy_img_name='augmentation_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
