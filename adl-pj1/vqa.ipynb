{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to images zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJMcisX_m37d",
    "outputId": "eea952d5-f441-47c0-fbf7-146114f153a3"
   },
   "outputs": [],
   "source": [
    "!tar -xvf  '/content/drive/MyDrive/Copy of nyu_depth_images.tar' -C './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv0BwQnKV5Cp",
    "outputId": "3deb2b5a-fc4a-4915-f9c3-3e918fd79f20"
   },
   "outputs": [],
   "source": [
    "!pip install -U keras\n",
    "!pip install transformers\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xx51KqLc0F2z",
    "outputId": "0af8376c-88e7-4681-c95f-2c8411f6ca98"
   },
   "outputs": [],
   "source": [
    "import os, re, random, zipfile, pickle, torch, logging\n",
    "import numpy as np, torch.nn as nn, pandas as pd,\\\n",
    "torch.nn.functional as F, matplotlib.pyplot as plt,\\\n",
    "seaborn as sn\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from time import time, sleep\n",
    "from models import SelfVQA\n",
    "from string import punctuation\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import (\n",
    "    TensorDataset,\n",
    "    Dataset,\n",
    "    DataLoader, \n",
    "    Subset, \n",
    "    RandomSampler, SequentialSampler\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from PIL import Image, ImageFilter, ImageDraw\n",
    "from torchvision.models import vgg16_bn\n",
    "from torchvision.transforms import ToTensor\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrvfA5p09NJO"
   },
   "outputs": [],
   "source": [
    "def download_dirs(dirlist):\n",
    "    from google.colab import files\n",
    "    for dirname in dirlist:\n",
    "        for filename in os.listdir(dirname):\n",
    "            filename = os.path.join(dirname, filename)\n",
    "            files.download(filename)\n",
    "            \n",
    "def get_dataLoader(dataset, splits, batch_sizes, shuffle_indices, **kwargs):\n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    if shuffle_indices:\n",
    "        np.random.shuffle(indices)\n",
    "    split_index = int(n_samples*splits[0])\n",
    "    train_indices = indices[:split_index]\n",
    "    valid_indices = indices[split_index:]\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, shuffle=True, batch_size=batch_sizes[0], **kwargs\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, shuffle=False, batch_size=batch_sizes[1], **kwargs\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def fix_seed(seed_value, random_lib=False, numpy_lib=False, torch_lib=False):\n",
    "    if random_lib:\n",
    "        random.seed(seed_value)\n",
    "    if numpy_lib:\n",
    "        np.random.seed(seed_value)\n",
    "    if torch_lib:\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def setup_logger(name, format=None, level=logging.DEBUG, handlers=None, log_file='default.log'):\n",
    "    logging.basicConfig(\n",
    "        level=level, \n",
    "        format=format if format else '%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=handlers if handlers else [\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "def cal_accuracy(pred_labels, true_labels):\n",
    "    _, pred_labels = pred_labels.max(dim=1)\n",
    "    true_labels = true_labels.view(-1)\n",
    "    return torch.sum(pred_labels == true_labels).item() / true_labels.size()[0]\n",
    "\n",
    "def cal_wups(pred_labels, true_labels, label2word):\n",
    "    print(pred_labels)\n",
    "    print(true_labels)\n",
    "    pred_labels = pred_labels.tolist()\n",
    "    true_labels = true_labels.tolist()\n",
    "    wups_scores = []\n",
    "    for i in range(len(pred_labels)):\n",
    "        pred_syn = wordnet.synsets(label2word[pred_labels[i]])\n",
    "        true_syn = wordnet.synsets(label2word[true_labels[i]])\n",
    "        mid_score = []\n",
    "        for psyn in pred_syn:\n",
    "            for tsyn in true_syn:\n",
    "                score = psyn.wup_similarity(tsyn)\n",
    "                if score:\n",
    "                    mid_score.append(score)\n",
    "        mean_score = np.mean(mid_score)\n",
    "        if not np.isnan(mean_score):\n",
    "            wups_scores.append(mean_score)\n",
    "    return np.mean(wups_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvz4oivi8vQ1"
   },
   "outputs": [],
   "source": [
    "class SelfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ant_df, images_dir, tokenizer=None, transform=ToTensor()):\n",
    "        self.ant_df = ant_df\n",
    "        self.encoded_matrix, self.vocab_size, self.tokenizer = encode_txt(\n",
    "            list(ant_df.question.values), tokenizer\n",
    "        )\n",
    "        self.encoded_matrix = torch.as_tensor(self.encoded_matrix)\n",
    "        self.image_names = list(ant_df.image_name.values)\n",
    "        self.images_dir = images_dir\n",
    "        self.labels = torch.as_tensor(ant_df.answer_id.values)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        question = self.encoded_matrix[item]\n",
    "        image_name = os.path.join(self.images_dir, self.image_names[item])\n",
    "        img = Image.open(image_name)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[item]\n",
    "        return question, img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class TrainTest():\n",
    "    def __init__(self, model, optimizer, scheduler, criterion, logger=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.logger = logger\n",
    "        self.tr_metrics = {\n",
    "            'train_loss':[],\n",
    "            'train_accuracy':[],\n",
    "            'valid_loss':[],\n",
    "            'valid_accuracy':[],\n",
    "        }\n",
    "        \n",
    "    def train(\n",
    "        self, train_loader, valid_loader, \n",
    "        num_epochs, device, eval_interval,\n",
    "        clip=None, model_path=None, save_per_epoch=None,\n",
    "        results_path=None, defaults=None, **kwargs\n",
    "    ):\n",
    "        total_itrs = num_epochs*len(train_loader)\n",
    "        num_tr, total_tr_loss, itr = 0, 0, 0\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (questions, imgs, labels) in enumerate(train_loader):\n",
    "                questions = questions.to(device)\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(\n",
    "                    questions,\n",
    "                    imgs\n",
    "                )\n",
    "                tr_loss = self.criterion(output, labels.view(-1))\n",
    "                # nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                if self.logger:\n",
    "                    self.logger.info(f'Training: {itr}/{total_itrs} -- loss: {tr_loss.item()}')\n",
    "                tr_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                num_tr += 1\n",
    "                total_tr_loss += tr_loss\n",
    "                if itr % eval_interval == 0 or itr+1 == total_itrs:\n",
    "                    self.tr_metrics['train_loss'].append(total_tr_loss.cpu().item()/num_tr)\n",
    "                    tr_accuracy = cal_accuracy(output, labels)\n",
    "                    self.tr_metrics['train_accuracy'].append(tr_accuracy)\n",
    "                    num_tr, total_tr_loss = 0, 0\n",
    "                    val_loss = 0\n",
    "                    self.model.eval()\n",
    "                    val_accuracy = []\n",
    "                    with torch.no_grad():\n",
    "                        for i, (val_questions, val_imgs, val_labels) in enumerate(valid_loader):\n",
    "                            val_questions = val_questions.to(device)\n",
    "                            val_imgs = val_imgs.to(device)\n",
    "                            val_labels = val_labels.to(device)\n",
    "                            self.optimizer.zero_grad()\n",
    "                            val_output = self.model(\n",
    "                                val_questions,\n",
    "                                val_imgs\n",
    "                            )\n",
    "                            val_loss += self.criterion(val_output, val_labels.view(-1))\n",
    "                            val_accuracy.append(cal_accuracy(val_output, val_labels))\n",
    "                    self.tr_metrics['valid_accuracy'].append(np.mean(val_accuracy))\n",
    "                    self.tr_metrics['valid_loss'].append(val_loss.cpu().item()/len(valid_loader))\n",
    "                    self.model.train()\n",
    "                    if self.logger:\n",
    "                        self.logger.info(f'Training: iteration: {itr}/{total_itrs} -- epoch: {epoch} -- '\n",
    "                        f' train_loss: {self.tr_metrics[\"train_loss\"][-1]:.3f} -- train_accuracy: {self.tr_metrics[\"train_accuracy\"][-1]:.2f}'\n",
    "                        f' valid_loss: {self.tr_metrics[\"valid_loss\"][-1]:.3f} -- valid_accuracy: {self.tr_metrics[\"valid_accuracy\"][-1]:.2f}')\n",
    "                itr += 1\n",
    "            if model_path and results_path and ((epoch+1) % save_per_epoch == 0) and epoch != 0:\n",
    "                self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_train')\n",
    "                self.save_results(results_path, f'{epoch+1}_epochs_train', self.tr_metrics)\n",
    "        if model_path and results_path:\n",
    "            self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_last_train')\n",
    "            self.save_results(results_path, f'{epoch+1}_epochs_last_train', self.tr_metrics)\n",
    "            \n",
    "    def save_model(self, epoch, model_path, name):\n",
    "        model_dir = '/'.join(model_path.split('/')[:-1])\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'loss': self.tr_metrics['valid_loss'][-1],\n",
    "            }, os.path.join(model_dir, f'model_{name}.pt')\n",
    "        )\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: model saved to: {model_dir}/model_{name}.pt')\n",
    "    \n",
    "    def save_results(self, results_path, name, results):\n",
    "        results_dir = '/'.join(results_path.split('/')[:-1])\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        with open(os.path.join(results_dir, f'results_{name}.pkl'), 'wb') as save_file:\n",
    "            pickle.dump(results, save_file)\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: results saved to: {results_dir}/resutls_{name}.pkl')\n",
    "\n",
    "    def test(self, test_loader, device, label2word, results_path=None, defaults=None):\n",
    "        test_accuracy, test_true, test_pred = [], [], []\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (test_questions, test_imgs, test_labels) in enumerate(test_loader):\n",
    "                test_questions = test_questions.to(device)\n",
    "                test_imgs = test_imgs.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                test_output = self.model(\n",
    "                    test_questions,\n",
    "                    test_imgs\n",
    "                )\n",
    "                test_loss += self.criterion(test_output, test_labels.view(-1))\n",
    "                test_accuracy.append(cal_accuracy(test_output, test_labels))\n",
    "                test_true.append(test_labels.cpu())\n",
    "                test_pred.append(test_output.cpu().max(dim=1)[1])\n",
    "        test_true = torch.cat(test_true)\n",
    "        test_pred = torch.cat(test_pred)\n",
    "        test_loss = test_loss.cpu().item()/len(test_loader)\n",
    "        test_accuracy = np.mean(test_accuracy)\n",
    "        test_wups = cal_wups(test_pred, test_true, label2word)\n",
    "        prf = precision_recall_fscore_support(\n",
    "            test_true,\n",
    "            test_pred,\n",
    "            labels=list(label2word.keys()),\n",
    "            average='weighted'\n",
    "        )\n",
    "        confm = confusion_matrix(test_true, test_pred, labels=list(label2word.keys()))\n",
    "        self.ts_metrics = {\n",
    "            'loss':test_loss,\n",
    "            'accuracy':test_accuracy,\n",
    "            'wups':test_wups,\n",
    "            'precision':prf[0],\n",
    "            'recall':prf[1],\n",
    "            'f1_score':prf[2],\n",
    "            'confusion_matrix':confm\n",
    "        }\n",
    "        if self.logger:\n",
    "            print(f'Testing: test_loss: {test_loss:.3f} -- test_accurcy: {test_accuracy:.2f} -- test_wups: {test_wups:.2f}')\n",
    "        if results_path:\n",
    "            self.save_results(results_path, f'test', self.ts_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2VLMy3c7C8A"
   },
   "outputs": [],
   "source": [
    "def resize_images(resize_shape, images_path, img_dist_path='rescaled_images'):\n",
    "    if not os.path.exists(img_dist_path):\n",
    "        os.makedirs(img_dist_path)\n",
    "    else: return img_dist_path\n",
    "    image_names = sorted(os.listdir(images_path))\n",
    "    img_counter = 0\n",
    "    for img_file in image_names:\n",
    "        img = Image.open(os.path.join(images_path, img_file))\n",
    "        img.thumbnail(resize_shape, Image.ANTIALIAS)\n",
    "        new_path = os.path.join(img_dist_path, img_file)\n",
    "        img.save(new_path, format='PNG')\n",
    "        img_counter += 1\n",
    "    return img_dist_path\n",
    "\n",
    "def create_df(file_path, answer_mapping=None):\n",
    "    with open(file_path, 'r') as annotation_file:\n",
    "        questions = []\n",
    "        answers = []\n",
    "        image_names = []\n",
    "        chars = set('_,')\n",
    "        lines = annotation_file.readlines()\n",
    "        for i in range(0, len(lines)-1, 2):\n",
    "            question = lines[i].strip().split()\n",
    "            answer = lines[i+1].strip()\n",
    "            if answer_mapping and answer not in answer_mapping.keys():\n",
    "                continue\n",
    "            if not any((c in chars) for c in answer):\n",
    "                image_name = question.pop(-2)\n",
    "                image_names.append(image_name[image_name.index('image'):]+'.png')\n",
    "                answers.append(answer)\n",
    "                questions.append(' '.join(question[:-3])+'?')\n",
    "    df = pd.DataFrame(\n",
    "        list(zip(image_names, questions, answers)), columns=['image_name', 'question', 'answer']\n",
    "    ).sort_values(['image_name']).reset_index(drop=True)\n",
    "    valid_answers = np.sort(df.answer.unique())\n",
    "    if not answer_mapping:\n",
    "        answer_mapping = dict(zip(valid_answers, range(len(valid_answers))))\n",
    "    df['answer_id'] = df.answer.apply(lambda x: answer_mapping[x])\n",
    "    return df, answer_mapping\n",
    "\n",
    "def encode_txt(txt_list, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        print('train tokenizer')\n",
    "        tokenizer = Tokenizer(oov_token='OOV')\n",
    "        tokenizer.fit_on_texts(txt_list)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    encoded_txt = tokenizer.texts_to_sequences(txt_list)\n",
    "    encoded_matrix = pad_sequences(\n",
    "        encoded_txt,\n",
    "        padding='pre',\n",
    "        truncating='pre'\n",
    "    )\n",
    "    return encoded_matrix, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2g9gu5TA1xl"
   },
   "outputs": [],
   "source": [
    "cuda_flag = True if torch.cuda.is_available() else False\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "n_iters = 10000\n",
    "num_epochs = 12\n",
    "eval_interval = 150\n",
    "save_model = True\n",
    "device = torch.device('cuda' if cuda_flag else 'cpu')\n",
    "params = {'num_workers': 2, 'pin_memory': True} if cuda_flag else {}\n",
    "data_splits = [0.9, 0.1]\n",
    "batch_sizes = [8, 8]\n",
    "seed = 20214\n",
    "fix_seed(seed, random_lib=True, numpy_lib=True, torch_lib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path to images directory and annotation files for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAuiTepM7C8F",
    "outputId": "123cb64d-e4bd-472a-82ce-3a1cc0f95098"
   },
   "outputs": [],
   "source": [
    "resize_shape = (200, 200)\n",
    "resized_dir = resize_images(resize_shape, './nyu_depth_images/', './rescaled_images/')\n",
    "tr_ant_path = './qa.894.raw.train.txt'\n",
    "train_df, answer_mapping = create_df(tr_ant_path)\n",
    "label2word = {v: k for k, v in answer_mapping.items()}\n",
    "train_dataset = SelfDataset(train_df, resized_dir)\n",
    "ts_ant_path = './qa.894.raw.test.txt'\n",
    "test_df, _ = create_df(ts_ant_path, answer_mapping)\n",
    "test_dataset = SelfDataset(test_df, resized_dir, train_dataset.tokenizer)\n",
    "\n",
    "train_loader, valid_loader = get_dataLoader(\n",
    "    train_dataset,\n",
    "    data_splits,\n",
    "    batch_sizes,\n",
    "    shuffle_indices=True, **params\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, shuffle=False, batch_size=8, **params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to GloVe 300d embedding fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD2iey147C8F"
   },
   "outputs": [],
   "source": [
    "embedded_words = dict()\n",
    "glove_path = '/content/drive/MyDrive/Copy of glove.42B.300d.txt'\n",
    "glove_file = open(glove_path)\n",
    "for line in glove_file:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    words_weights = np.array(values[1:], dtype='float32')\n",
    "    embedded_words[word] = words_weights\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqsWTz0T7C8G"
   },
   "outputs": [],
   "source": [
    "mean_embedding = np.array(list(embedded_words.values())).mean(axis=0)\n",
    "embedding_dim = len(embedded_words[next(iter(embedded_words))])\n",
    "embedding_matrix = np.zeros((train_dataset.vocab_size, embedding_dim))\n",
    "for word, index in train_dataset.tokenizer.word_index.items():\n",
    "    embedding_vector = embedded_words.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[index] = mean_embedding\n",
    "\n",
    "del embedded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HG_U5XhY7C8G",
    "outputId": "b50a2c93-3972-4b89-a214-51195d2a9e69"
   },
   "outputs": [],
   "source": [
    "output_size = len(answer_mapping)\n",
    "hidden_dim = 150\n",
    "n_layers = 1\n",
    "lstm_drop = 0.5\n",
    "\n",
    "model = SelfVQA(\n",
    "    train_dataset.vocab_size,\n",
    "    output_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    lstm_drop,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "embedding_matrix = torch.as_tensor(embedding_matrix, dtype=torch.float)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "model.embedding.weight.requires_grad=False\n",
    "for param in model.vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kceZqSSm7C8K"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = SGD(model.parameters(), lr=lrmain)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=1)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_epochs*len(train_loader),\n",
    ")\n",
    "open('metrics.log', 'w').close()\n",
    "logger = setup_logger(name='track_logger', level=logging.INFO, log_file='metrics.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIp1i2JfDVzV",
    "outputId": "e17d3189-1198-4903-98d6-f0d4ce87be6c"
   },
   "outputs": [],
   "source": [
    "traintest = TrainTest(model, optimizer, scheduler, F.nll_loss, logger)\n",
    "traintest.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    eval_interval,\n",
    "    model_path='./models/',\n",
    "    save_per_epoch=10,\n",
    "    results_path='./results/',\n",
    "    clip=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAz26kdhiX2O",
    "outputId": "77047bee-94ea-409d-df3e-944b4a66c7bb"
   },
   "outputs": [],
   "source": [
    "traintest.test(test_loader, device, label2word, results_path='./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16
    },
    "id": "9VWUWEJNvQxn",
    "outputId": "09034386-5d51-4cfb-fb6d-98f5c1aab9f4"
   },
   "outputs": [],
   "source": [
    "download_dirs(['./results/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Z2dv2vPiO5c",
    "outputId": "af2883f7-cc72-4a9d-f673-195b1fe86977"
   },
   "outputs": [],
   "source": [
    "traintest.tr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe4mSn5ojNp1",
    "outputId": "c91cd914-b778-4267-d6f0-a76c6d0a2bb2"
   },
   "outputs": [],
   "source": [
    "answer_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj-dK6DmJZ4U"
   },
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    results_dir, train_results_file, eval_interval, just_train=False,\n",
    "    test_results_file=None, class_mapping=None, **kwargs\n",
    "):\n",
    "    with open(os.path.join(results_dir, train_results_file), 'rb') as results_file:\n",
    "        train_results = pickle.load(results_file)\n",
    "    iterations = [i*eval_interval for i in range(len(train_results['train_loss']))]\n",
    "    plt.plot(iterations, train_results['train_loss'], label=f'train loss')\n",
    "    plt.plot(iterations, train_results['valid_loss'], label=f'valid loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'train-validation-loss.png'), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.figure()\n",
    "    plt.plot(iterations, train_results['train_accuracy'], label=f'train accuracy')\n",
    "    plt.plot(iterations, train_results['valid_accuracy'], label=f'valid accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'train-validation-accuracy.png'), dpi=300, bbox_inches=\"tight\")\n",
    "    if not just_train:\n",
    "        with open(os.path.join(results_dir, test_results_file), 'rb') as results_file:\n",
    "            test_results = pickle.load(results_file)\n",
    "        class_mapping = sorted(\n",
    "            list(class_mapping.items()), key=lambda x: x[1]\n",
    "        )\n",
    "        classes = [x[0] for x in class_mapping]\n",
    "        plt.figure()\n",
    "        test_confm = pd.DataFrame(test_results['confusion_matrix'], classes, classes)\n",
    "        sn.set(font_scale=1)\n",
    "        sn.heatmap(\n",
    "            test_confm, \n",
    "            annot=kwargs['conf_annot'] if 'conf_annot' in kwargs else True, \n",
    "            annot_kws={\"size\": kwargs['font-size'] if 'font-size' in kwargs else 10}\n",
    "        )\n",
    "        plt.autoscale(True)\n",
    "        plt.savefig(os.path.join(results_dir, 'test-confusion-matrix.png'), dpi=300, bbox_inches=\"tight\")\n",
    "        print(f'{\"*\"*20} Test Metrics: {\"*\"*20}\\n'\n",
    "              f'Loss: {test_results[\"loss\"]:.3f}\\n'\n",
    "              f'Accuracy: {test_results[\"accuracy\"]:.3f}\\n'\n",
    "              f'WUPS: {test_results[\"wups\"]:.3f}\\n'\n",
    "              f'Weighted Precision: {test_results[\"precision\"]:.3f}\\n'\n",
    "              f'Weighted Recall: {test_results[\"recall\"]:.3f}\\n'\n",
    "              f'Weighted F1-score: {test_results[\"f1_score\"]:.3f}\\n'\n",
    "              f'{\"*\"*55}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'VQA-results'\n",
    "test_results_file = 'results_test.pkl'\n",
    "train_results_file = 'results_12_epochs_last_train.pkl'\n",
    "report_results(\n",
    "    results_dir, train_results_file, eval_interval,\n",
    "    test_results_file=test_results_file,\n",
    "    class_mapping=dict(zip(range(len(answer_mapping)), range(len(answer_mapping)))), conf_annot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vqa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
