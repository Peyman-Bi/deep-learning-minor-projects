{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kq0iFQNITH9H",
    "outputId": "ef9f17a1-d53d-4cad-f9e1-dc64a8dceb2f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-YNezgX9TmRa",
    "outputId": "9f689bc5-7de3-4962-dd16-c0c18f2d2e21"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PIo050fPwVr"
   },
   "outputs": [],
   "source": [
    "import torch, logging, os, re, random, pickle, logging\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, \n",
    "    DataLoader, \n",
    "    Subset, \n",
    "    RandomSampler, \n",
    "    SequentialSampler, \n",
    "    Dataset\n",
    ")\n",
    "import numpy as np, torch.nn as nn, pandas as pd,\\\n",
    "torch.nn.functional as F, matplotlib.pyplot as plt,\\\n",
    "seaborn as sn\n",
    "from time import time, sleep\n",
    "from models import BertForRE\n",
    "from torch.optim import Adam\n",
    "from string import punctuation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, \n",
    "    DataLoader, \n",
    "    Subset, \n",
    "    RandomSampler, \n",
    "    SequentialSampler, \n",
    "    Dataset\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m54fC5YfPwVy"
   },
   "outputs": [],
   "source": [
    "def download_dirs(dirlist):\n",
    "    from google.colab import files\n",
    "    for dirname in dirlist:\n",
    "        for filename in os.listdir(dirname):\n",
    "            filename = os.path.join(dirname, filename)\n",
    "            files.download(filename)\n",
    "\n",
    "def get_dataLoader(dataset, splits, batch_sizes, shuffle_indices, **kwargs):\n",
    "    n_samples = len(dataset)\n",
    "    indices = list(range(n_samples))\n",
    "    if shuffle_indices:\n",
    "        np.random.shuffle(indices)\n",
    "    split_index = int(n_samples*splits[0])\n",
    "    train_indices = indices[:split_index]\n",
    "    valid_indices = indices[split_index:]\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    valid_dataset = Subset(dataset, valid_indices)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, shuffle=True, batch_size=batch_sizes[0], **kwargs\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, shuffle=False, batch_size=batch_sizes[1], **kwargs\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def fix_seed(seed_value, random_lib=False, numpy_lib=False, torch_lib=False):\n",
    "    if random_lib:\n",
    "        random.seed(seed_value)\n",
    "    if numpy_lib:\n",
    "        np.random.seed(seed_value)\n",
    "    if torch_lib:\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def setup_logger(name, format=None, level=logging.DEBUG, handlers=None, log_file='default.log'):\n",
    "    logging.basicConfig(\n",
    "        level=level, \n",
    "        format=format if format else '%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=handlers if handlers else [\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "def cal_accuracy(pred_labels, true_labels):\n",
    "    _, pred_labels = pred_labels.max(dim=1)\n",
    "    true_labels = true_labels.view(-1)\n",
    "    return torch.sum(pred_labels == true_labels).item() / true_labels.size()[0]\n",
    "\n",
    "\n",
    "def get_df(fliepath, label_dict=None, n_phrases=0):\n",
    "    rep = {'<e1>': ' <E1> ', '<e2>': ' <E2> ', '</e1>': ' </E1> ', '</e2>': ' </E2> '}\n",
    "    rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    with open(fliepath, 'r') as train_file:\n",
    "        phrases = []\n",
    "        labels = []\n",
    "        for line in train_file.readlines():\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line)>1:\n",
    "                phrases.append(pattern.sub(lambda m: rep[re.escape(m.group(0))], line[1].strip('\"')))\n",
    "            elif not line[0].startswith('Comment') and line[0]:\n",
    "                labels.append(line[0].split('(')[0])\n",
    "    if not label_dict:\n",
    "        label_set = set(labels[:n_phrases])-{'Other'}\n",
    "        label_dict = dict(zip(label_set, range(len(label_set))))\n",
    "        label_dict['Other'] = len(label_set)\n",
    "    df = pd.DataFrame(list(zip(phrases[:n_phrases], labels[:n_phrases])), columns=['phrase', 'label'])\n",
    "    df['label_id'] = df.label.apply(lambda x: label_dict[x])\n",
    "    df['phrase_len'] = df.phrase.apply(lambda x: len(x.split()))\n",
    "    return df, label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XktuFP19mCQJ"
   },
   "outputs": [],
   "source": [
    "class SelfDataset(Dataset):\n",
    "    def __init__(self, input_df, batch_size, tokenizer=None, special_tokens=['<E1>', '</E1>', '<E2>', '</E2>']):\n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                'bert-base-uncased',\n",
    "                do_lower_case = True,\n",
    "                additional_special_tokens = special_tokens\n",
    "            )\n",
    "        self.inputs = list(input_df.phrase.values)\n",
    "        self.special_token_ids = self.tokenizer.convert_tokens_to_ids(['<E1>', '</E1>', '<E2>', '</E2>'])\n",
    "        self.labels = torch.as_tensor(\n",
    "            input_df.label_id.values, dtype = torch.long\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.inputs_size = len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        encoded_input = self.tokenizer(\n",
    "            self.inputs[\n",
    "                item*self.batch_size:min(item*self.batch_size+self.batch_size, self.inputs_size)\n",
    "            ],\n",
    "            add_special_tokens = True,\n",
    "            padding='longest',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "        )\n",
    "        special_indices = torch.as_tensor(np.where(\n",
    "            np.isin(encoded_input.input_ids.numpy(), self.special_token_ids)\n",
    "        )[1]).view(encoded_input.input_ids.size(0), 2, 2)\n",
    "        return (\n",
    "            encoded_input.input_ids, \n",
    "            encoded_input.attention_mask, \n",
    "            self.labels[\n",
    "                item*self.batch_size:min(item*self.batch_size+self.batch_size, self.inputs_size)\n",
    "            ],\n",
    "            special_indices\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.inputs_size/self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFxMVgwoPwV4"
   },
   "outputs": [],
   "source": [
    "class TrainTest():\n",
    "    def __init__(self, model, optimizer, scheduler, criterion, logger=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.logger = logger\n",
    "        self.tr_metrics = {\n",
    "            'train_loss':[],\n",
    "            'train_accuracy':[],\n",
    "            'valid_loss':[],\n",
    "            'valid_accuracy':[],\n",
    "        }\n",
    "        \n",
    "    def train(\n",
    "        self, train_loader, valid_loader, \n",
    "        num_epochs, device, eval_interval, \n",
    "        clip=None, model_path=None, save_per_epoch=None,\n",
    "        results_path=None, defaults=None, **kwargs\n",
    "    ):\n",
    "        total_itrs = num_epochs*len(train_loader)\n",
    "        num_tr, total_tr_loss, itr = 0, 0, 0\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (input_ids, att_masks, input_labels, special_indices) in enumerate(train_loader):\n",
    "                input_ids = input_ids.squeeze(0).to(device)\n",
    "                att_masks = att_masks.squeeze(0).to(device)\n",
    "                input_labels = input_labels.squeeze(0).to(device)\n",
    "                special_indices = special_indices.squeeze(0).to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(\n",
    "                    input_ids,\n",
    "                    attention_mask=att_masks,\n",
    "                    firsts=special_indices[:, 0, :],\n",
    "                    seconds=special_indices[:, 1, :],\n",
    "                    token_type_ids=None\n",
    "                )\n",
    "                tr_loss = self.criterion(output, input_labels.view(-1))\n",
    "                # nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                if self.logger:\n",
    "                    self.logger.info(f'Training: {itr}/{total_itrs} -- loss: {tr_loss.item()}')\n",
    "                tr_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                num_tr += 1\n",
    "                total_tr_loss += tr_loss\n",
    "                if itr % eval_interval == 0 or itr+1 == total_itrs:\n",
    "                    self.tr_metrics['train_loss'].append(total_tr_loss.cpu().item()/num_tr)\n",
    "                    tr_accuracy = cal_accuracy(output, input_labels)\n",
    "                    self.tr_metrics['train_accuracy'].append(tr_accuracy)\n",
    "                    num_tr, total_tr_loss = 0, 0\n",
    "                    val_loss = 0\n",
    "                    self.model.eval()\n",
    "                    val_accuracy = []\n",
    "                    with torch.no_grad():\n",
    "                        for val_ids, val_masks, val_labels, val_tokens in valid_loader:\n",
    "                            val_ids = val_ids.squeeze(0).to(device)\n",
    "                            val_masks = val_masks.squeeze(0).to(device)\n",
    "                            val_labels = val_labels.squeeze(0).to(device)\n",
    "                            val_tokens = val_tokens.squeeze(0).to(device)\n",
    "                            val_output = self.model(\n",
    "                                val_ids,\n",
    "                                attention_mask=val_masks,\n",
    "                                firsts=val_tokens[:, 0, :],\n",
    "                                seconds=val_tokens[:, 1, :],\n",
    "                                token_type_ids=None\n",
    "                            )\n",
    "                            val_loss += self.criterion(val_output, val_labels.view(-1))\n",
    "                            val_accuracy.append(cal_accuracy(val_output, val_labels))\n",
    "                    self.tr_metrics['valid_accuracy'].append(np.mean(val_accuracy))\n",
    "                    self.tr_metrics['valid_loss'].append(val_loss.cpu().item()/len(valid_loader))\n",
    "                    self.model.train()\n",
    "                    if self.logger:\n",
    "                        self.logger.info(f'Training: iteration: {itr}/{total_itrs} -- epoch: {epoch} -- '\n",
    "                        f' train_loss: {self.tr_metrics[\"train_loss\"][-1]:.3f} -- train_accuracy: {self.tr_metrics[\"train_accuracy\"][-1]:.2f}'\n",
    "                        f' valid_loss: {self.tr_metrics[\"valid_loss\"][-1]:.3f} -- valid_accuracy: {self.tr_metrics[\"valid_accuracy\"][-1]:.2f}')\n",
    "                itr += 1\n",
    "            if model_path and results_path and ((epoch+1) % save_per_epoch == 0) and epoch != 0:\n",
    "                self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_train')\n",
    "                self.save_results(results_path, f'{epoch+1}_epochs_train', self.tr_metrics)\n",
    "        if model_path and results_path:\n",
    "            self.save_model(epoch+1, model_path, f'{epoch+1}_epochs_last_train')\n",
    "            self.save_results(results_path, f'{epoch+1}_epochs_last_train', self.tr_metrics)\n",
    "            \n",
    "    def save_model(self, epoch, model_path, name):\n",
    "        model_dir = '/'.join(model_path.split('/')[:-1])\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'loss': self.tr_metrics['valid_loss'][-1],\n",
    "            }, os.path.join(model_dir, f'model_{name}.pt')\n",
    "        )\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: model saved to: {model_dir}/model_{name}.pt')\n",
    "    \n",
    "    def save_results(self, results_path, name, results):\n",
    "        results_dir = '/'.join(results_path.split('/')[:-1])\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        with open(os.path.join(results_dir, f'results_{name}.pkl'), 'wb') as save_file:\n",
    "            pickle.dump(results, save_file)\n",
    "        if self.logger:\n",
    "            self.logger.info(f'Training: results saved to: {results_dir}/resutls_{name}.pkl')\n",
    "\n",
    "    def test(self, test_loader, device, all_labels, results_path=None, defaults=None):\n",
    "        test_accuracy, test_true, test_pred = [], [], []\n",
    "        test_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_ids, test_masks, test_labels, test_tokens in test_loader:\n",
    "                test_ids = test_ids.squeeze(0).to(device)\n",
    "                test_masks = test_masks.squeeze(0).to(device)\n",
    "                test_labels = test_labels.squeeze(0).to(device)\n",
    "                test_tokens = test_tokens.squeeze(0).to(device)\n",
    "                test_output = self.model(\n",
    "                    test_ids,\n",
    "                    attention_mask=test_masks,\n",
    "                    firsts=test_tokens[:, 0, :],\n",
    "                    seconds=test_tokens[:, 1, :],\n",
    "                    token_type_ids=None\n",
    "                )\n",
    "                test_loss += self.criterion(test_output, test_labels.view(-1))\n",
    "                test_accuracy.append(cal_accuracy(test_output, test_labels))\n",
    "                test_true.append(test_labels.cpu())\n",
    "                test_pred.append(test_output.cpu().max(dim=1)[1])\n",
    "        test_true = torch.cat(test_true)\n",
    "        test_pred = torch.cat(test_pred)\n",
    "        test_loss = test_loss.cpu().item()/len(test_loader)\n",
    "        test_accuracy = np.mean(test_accuracy)\n",
    "        prf = precision_recall_fscore_support(\n",
    "            test_true,\n",
    "            test_pred,\n",
    "            labels=all_labels,\n",
    "            average='weighted'\n",
    "        )\n",
    "        confm = confusion_matrix(test_true, test_pred, labels=all_labels)\n",
    "        self.ts_metrics = {\n",
    "            'loss':test_loss,\n",
    "            'accuracy':test_accuracy,\n",
    "            'precision':prf[0],\n",
    "            'recall':prf[1],\n",
    "            'f1_score':prf[2],\n",
    "            'confusion_matrix':confm\n",
    "        }\n",
    "        if self.logger:\n",
    "            print(f'Testing: test_loss: {test_loss:.3f} -- test_accurcy: {test_accuracy:.2f}')\n",
    "        if results_path:\n",
    "            self.save_results(results_path, f'test', self.ts_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Lffwg2PwV7"
   },
   "outputs": [],
   "source": [
    "cuda_flag = True if torch.cuda.is_available() else False\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "n_iters = 10000\n",
    "num_epochs = 10\n",
    "eval_interval = 40\n",
    "save_model = True\n",
    "device = torch.device('cuda' if cuda_flag else 'cpu')\n",
    "params = {'num_workers': 2, 'pin_memory': True} if cuda_flag else {}\n",
    "data_splits = [0.85, 0.15]\n",
    "batch_sizes = [1, 1]\n",
    "seed = 20214\n",
    "fix_seed(seed, random_lib=True, numpy_lib=True, torch_lib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set path to train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnGQFNU1PwV9",
    "outputId": "739c79a2-2d6e-4cd3-dabf-ceb7b3d93408"
   },
   "outputs": [],
   "source": [
    "train_df, train_label_dict = get_df(\n",
    "    '/content/drive/MyDrive/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT', \n",
    "    n_phrases=7109\n",
    ")\n",
    "train_dataset = SelfDataset(train_df, batch_size=64)\n",
    "train_loader, valid_loader = get_dataLoader(\n",
    "    train_dataset,\n",
    "    data_splits,\n",
    "    batch_sizes,\n",
    "    shuffle_indices=True, **params\n",
    ")\n",
    "\n",
    "test_df, _ = get_df(\n",
    "    '/content/drive/MyDrive/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT', \n",
    "    label_dict=train_label_dict,\n",
    "    n_phrases=2717\n",
    ")\n",
    "test_dataset = SelfDataset(\n",
    "    test_df, batch_size=64, tokenizer=train_dataset.tokenizer\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, shuffle=False, batch_size=1, **params\n",
    ")\n",
    "\n",
    "print(f'number of epochs: {num_epochs}')\n",
    "print(f'number of iteration: {num_epochs*len(train_loader)}')\n",
    "print(f'number of intervals: {num_epochs*len(train_loader)/eval_interval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtB2kTvBPwV-"
   },
   "outputs": [],
   "source": [
    "vocab_file = 'bert-base-uncased'\n",
    "model = BertForRE(\n",
    "    vocab_file,\n",
    "    num_classes=len(train_label_dict),\n",
    "    fcc_uotput_size=128,\n",
    "    vocab_size=len(train_dataset.tokenizer)).to(device\n",
    ")\n",
    "optimizer = Adam([\n",
    "    {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "    {\"params\":model.dropout.parameters(), \"lr\": lrlast},\n",
    "    {\"params\":model.classifier.parameters(), \"lr\": lrlast},                    \n",
    "])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_epochs*len(train_loader)\n",
    ")\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "open('metrics.log', 'w').close()\n",
    "logger = setup_logger(name='track_logger', level=logging.INFO, log_file='metrics.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kC46UZpTBth4",
    "outputId": "18c1c508-737e-4a0e-8c61-763aedfc6702"
   },
   "outputs": [],
   "source": [
    "traintest = TrainTest(model, optimizer, scheduler, F.nll_loss, logger)\n",
    "traintest.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    eval_interval,\n",
    "    model_path='./models/',\n",
    "    save_per_epoch=4,\n",
    "    results_path='./results/',\n",
    "    clip=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "QHvdb-rknI2E",
    "outputId": "547faeac-182a-4c56-e2c9-c0276c1c2b2d"
   },
   "outputs": [],
   "source": [
    "traintest.test(\n",
    "    test_loader, \n",
    "    device, \n",
    "    list(train_label_dict.values()),\n",
    "    results_path='./results/'\n",
    ")\n",
    "download_dirs(['./results/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vR8R2onqEfTh",
    "outputId": "a1c3f65b-7ce0-4dbf-ece5-94bb90413445"
   },
   "outputs": [],
   "source": [
    "traintest.tr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(\n",
    "    results_dir, train_results_file, eval_interval, just_train=False,\n",
    "    test_results_file=None, class_mapping=None, loss_img_name='',\n",
    "    accuracy_img_name='', conf_mat_name='', **kwargs\n",
    "):\n",
    "    with open(os.path.join(results_dir, train_results_file), 'rb') as results_file:\n",
    "        train_results = pickle.load(results_file)\n",
    "    iterations = [i*eval_interval for i in range(len(train_results['train_loss']))]\n",
    "    plt.plot(iterations, train_results['train_loss'], label=f'train loss')\n",
    "    plt.plot(iterations, train_results['valid_loss'], label=f'valid loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, loss_img_name+'train-validation-loss.png'), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.figure()\n",
    "    plt.plot(iterations, train_results['train_accuracy'], label=f'train accuracy')\n",
    "    plt.plot(iterations, train_results['valid_accuracy'], label=f'valid accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, accuracy_img_name+'train-validation-accuracy.png'), dpi=300, bbox_inches=\"tight\")\n",
    "    if not just_train:\n",
    "        with open(os.path.join(results_dir, test_results_file), 'rb') as results_file:\n",
    "            test_results = pickle.load(results_file)\n",
    "        class_mapping = sorted(\n",
    "            list(class_mapping.items()), key=lambda x: x[1]\n",
    "        )\n",
    "        classes = [x[0] for x in class_mapping]\n",
    "        plt.figure()\n",
    "        test_confm = pd.DataFrame(test_results['confusion_matrix'], classes, classes)\n",
    "        sn.set(font_scale=1)\n",
    "        sn.heatmap(\n",
    "            test_confm, \n",
    "            annot=kwargs['conf_annot'] if 'conf_annot' in kwargs else True, \n",
    "            annot_kws={\"size\": kwargs['font-size'] if 'font-size' in kwargs else 10}\n",
    "        )\n",
    "        plt.autoscale(True)\n",
    "        plt.savefig(os.path.join(results_dir, conf_mat_name+'test-confusion-matrix.png'), dpi=300, bbox_inches=\"tight\")\n",
    "        print(f'{\"*\"*20} Test Metrics: {\"*\"*20}\\n'\n",
    "              f'Loss: {test_results[\"loss\"]:.3f}\\n'\n",
    "              f'Accuracy: {test_results[\"accuracy\"]:.3f}\\n'\n",
    "              f'Weighted Precision: {test_results[\"precision\"]:.3f}\\n'\n",
    "              f'Weighted Recall: {test_results[\"recall\"]:.3f}\\n'\n",
    "              f'Weighted F1-score: {test_results[\"f1_score\"]:.3f}\\n'\n",
    "              f'{\"*\"*55}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAszfJs7OqCg"
   },
   "outputs": [],
   "source": [
    "results_dir = 'RE-results'\n",
    "test_results_file = 'results_test.pkl'\n",
    "train_results_file = 'results_10_epochs_last_train.pkl'\n",
    "report_results(\n",
    "    results_dir, train_results_file, eval_interval,\n",
    "    test_results_file=test_results_file,\n",
    "    class_mapping=train_label_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "relation_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
